{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a598806-497b-4759-adfe-327f5c2e6d77",
   "metadata": {},
   "source": [
    "# Data analysis and processing\n",
    "\n",
    "We're going to re-analyse the data from the paper [Compact, low-threshold squeezed light source](https://opg.optica.org/abstract.cfm?uri=oe-27-26-37877), as well as data taken during the project that led to the paper [Distributed quantum sensing in a continuous-variable entangled network](https://www.nature.com/articles/s41567-019-0743-x).\n",
    "\n",
    "Both of these papers are about [squeezed light](https://omni.wikiwand.com/en/articles/Squeezed_states_of_light) generated by an [optical parametric oscillator (OPO)](https://omni.wikiwand.com/en/articles/Optical_parametric_oscillator). In the textbook description of squeezed vacuum, the variance of the squeezed quadrature (let's say it's $\\hat x$) is $\\text{Var}(x) = e^{-2r}$ and for the anti-squeezed quadrature ($\\hat p$) it is $\\text{Var}(p) = e^{2r}$. Here, $r$ is the _squeezing parameter_. For a realistic state, the situation is more complex - $r$ is not well-defined in the experiment, but the variance will instead depend on losses, the frequency that it is observed at, etc.\n",
    "\n",
    "Denoting by $V_+$ the variance of the anti-squeezing relative to shotnoise (the standard noise level, whose value depends on the definition of the quadrature operators), and by $V_-$ the squeezing variance, the simplest formula for the squeezing of an OPO is for the case of no losses and at DC (i.e. frequency = zero):\n",
    "\n",
    "$V_\\pm = 1 \\pm \\frac{4x}{(1\\mp x)^2},$\n",
    "\n",
    "where $x=\\sqrt{P/P_{th}}$ is the _pump parameter_ given by the power $P$ of the field used for pumping the nonlinear interaction inside the OPO relative to a _threshold power_ $P_{th}$.\n",
    "\n",
    "For non-zero losses, we have the _overall efficiency_ $\\eta = 1-\\text{losses}$, and we also introduce the _OPO half-width-half-max bandwidth_ $f_{HWHM}$ and the frequency of observation $f$. Then the expression above becomes\n",
    "\n",
    "$V_\\pm = 1 \\pm \\frac{4x\\eta}{(1\\mp x)^2 + (f/f_{HWHM})^2}.$\n",
    "\n",
    "The variance of a general quadrature $x_\\theta$ at an angle $\\theta$ with the squeezed quadrature can be obtained by\n",
    "\n",
    "$V(\\theta) = V_-\\cos^2\\theta + V_+\\sin^2\\theta$.\n",
    "\n",
    "Finally, if there is some fluctuation in the observed phase, this can be incorporated by modeling it as a normal distribution around the phase set point with standard deviation $\\sigma$, resulting in a variance\n",
    "\n",
    "$V(\\theta) = \\frac{1}{2} V_- (1+\\cos(2\\theta)e^{-2\\sigma^2}) + \\frac{1}{2} V_+ (1-\\cos(2\\theta)e^{-2\\sigma^2}) .$\n",
    "\n",
    "#### Data loading and processing\n",
    "\n",
    "The data for the first paper (the \"Jens\" paper) has been acquired by a Keysight N9000A signal analyzer. It is in CSV format which is easy to import e.g. with NumPy's `genfromtxt` function.\n",
    "\n",
    "The data for the second paper (the \"Xueshi\" paper) has been acquired by a LeCroy HDO6034 oscilloscope. This is in a proprietary binary format, so here we need some custom code to import it. This is available in the `lecroy.py` file.\n",
    "\n",
    "A better way to store large amounts of data is in the commonly used [HDF5 binary data format](https://omni.wikiwand.com/en/articles/Hierarchical_Data_Format). In Python, this can be used via the [h5py](https://www.h5py.org) package.\n",
    "\n",
    "We will also look at processing the data. For calculating spectra, filtering time series, etc., we will use tools from `scipy.signal` and `scipy.fft`. For fitting data to theoretical models, we will use tools from `scipy.optimize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7477265-114b-441b-9387-24498a130bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import lecroy\n",
    "import h5py\n",
    "import os\n",
    "import sys\n",
    "from scipy.signal import periodogram, welch\n",
    "from scipy.fft import rfft, irfft, rfftfreq\n",
    "from scipy.optimize import curve_fit, least_squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572cb325-5f03-4533-a6c0-d545da90a79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "jens_folder = 'test_data/jens'\n",
    "jens_files = os.listdir(jens_folder)\n",
    "xueshi_folder = 'test_data/xueshi'\n",
    "xueshi_files = os.listdir(xueshi_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be53a230-51c5-451a-8ecb-294ee301c918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin2db(lin):\n",
    "    return 10 * np.log10(lin)\n",
    "\n",
    "def db2lin(db):\n",
    "    return 10**(db/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187e7777-c88c-435e-8071-a34b348fe43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Vsq = lambda x, eta, frel: 1 - 4*eta*x / ((1+x)**2 + frel**2)\n",
    "Vasq = lambda x, eta, frel: 1 + 4*eta*x / ((1-x)**2 + frel**2)\n",
    "Vrot = lambda x, eta, frel, theta: np.cos(theta*np.pi/180)**2 * Vsq(x, eta, frel) + np.sin(theta*np.pi/180)**2 * Vasq(x, eta, frel)\n",
    "\n",
    "def Vrotfluct(x, eta, frel, theta, sigma_x=0, sigma_p=None):\n",
    "    if sigma_p is None:\n",
    "        sigma_p = sigma_x\n",
    "        \n",
    "    Vx = .5 * Vsq(x, eta, frel) * (1 + np.cos(2 * theta * np.pi/180) * np.exp(-2 * (sigma_x * np.pi/180)**2))\n",
    "    Vp = .5 * Vasq(x, eta, frel) * (1 - np.cos(2 * theta * np.pi/180) * np.exp(-2 * (sigma_p * np.pi/180)**2))\n",
    "\n",
    "    return Vx + Vp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cc0229-926b-4d7d-9598-cbd879271608",
   "metadata": {},
   "source": [
    "## Jens data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2f2163-ddde-4d99-b6df-50f9e6a3c8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "jens_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b348d0b8-a685-48d6-b028-66d7e36d6f5c",
   "metadata": {},
   "source": [
    "We select the files containing the spectrum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661a71cb-885b-4295-b643-0f64e1869322",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_spec_files = [os.path.join(jens_folder, f) for f in sorted(jens_files) if 'SPEC' in f and '_S' in f]\n",
    "asq_spec_files = [os.path.join(jens_folder, f) for f in sorted(jens_files) if 'SPEC' in f and '_A' in f]\n",
    "elec_spec_file = os.path.join(jens_folder, 'SPEC_elec.csv')\n",
    "vac_spec_file = os.path.join(jens_folder, 'SPEC_vac.csv')\n",
    "sq_spec_files, asq_spec_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ed734b-7a2a-48ae-af70-77ca44f8f5bf",
   "metadata": {},
   "source": [
    "Read data from the files. The first 45 lines of each file containt metadata, so we first look at these, but then skip them when reading the actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390853dc-610d-4838-a651-ef4658423ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(elec_spec_file) as f:\n",
    "    meta = [f.readline().strip().split(',') for i in range(45)]\n",
    "for text in meta:\n",
    "    if len(text) > 1:\n",
    "        print(f'{text[0]:<25} : {text[1]}')\n",
    "    else:\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08831daa-27c4-429f-828b-7076c062d20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "electronic = np.genfromtxt(elec_spec_file, delimiter=',', skip_header=45)\n",
    "vacuum = np.genfromtxt(vac_spec_file, delimiter=',', skip_header=45)\n",
    "squeezing = np.array([np.genfromtxt(f, delimiter=',', skip_header=45) for f in sq_spec_files])\n",
    "antisqueezing = np.array([np.genfromtxt(f, delimiter=',', skip_header=45) for f in asq_spec_files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad2a6d7-057b-4f6a-ba7e-e25a2d56a2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "electronic.shape, vacuum.shape, squeezing.shape, antisqueezing.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c489a342-76b7-4a75-a5d4-175e5503d996",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = electronic[:,0] / 1e6  # to MHz\n",
    "electronic = electronic[:,1]\n",
    "vacuum = vacuum[:,1]\n",
    "squeezing = squeezing[:,:,1]\n",
    "antisqueezing = antisqueezing[:,:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e487b1-e468-4da7-81ed-af8b3416070b",
   "metadata": {},
   "source": [
    "As we saw from the \"Y Axis Units\" in the meta data, the spectrum is already given on a logarithmic scale. We also see this when plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f649c2aa-4db5-4f51-b296-9ac079ddd91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(f, electronic, c='k')\n",
    "plt.plot(f, vacuum, c='C2')\n",
    "for sq in squeezing:\n",
    "    plt.plot(f, sq, c='C0')\n",
    "for asq in antisqueezing:\n",
    "    plt.plot(f, asq, c='C1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee90f25-687f-4ab8-843b-9f91b711c745",
   "metadata": {},
   "source": [
    "We are interested in the squeezing and antisqueezing noise variance _relative_ to the vacuum noise, so we normalise to this.\n",
    "Before doing this, we can subtract the electronic noise which is common for all traces. When performing arithmetics on the spectra, we must first convert from logarithmic to linear scale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beffb00-e8df-4fe6-bd02-8f192fca8796",
   "metadata": {},
   "outputs": [],
   "source": [
    "vacuum_corr_lin = db2lin(vacuum) - db2lin(electronic)\n",
    "squeezing_norm = lin2db((db2lin(squeezing) - db2lin(electronic)) / vacuum_corr_lin)\n",
    "antisqueezing_norm = lin2db((db2lin(antisqueezing) - db2lin(electronic)) / vacuum_corr_lin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07252640-9297-4345-ac3d-49b7c9b8cf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sq in squeezing_norm:\n",
    "    plt.plot(f, sq, c='C0')\n",
    "for asq in antisqueezing_norm:\n",
    "    plt.plot(f, asq, c='C1')\n",
    "\n",
    "plt.ylim(-9,16)\n",
    "plt.xlim(0, 120)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f462d6-6e9e-42c5-ab1a-00647a4ed3cc",
   "metadata": {},
   "source": [
    "Let's now try to fit the model outlined at the top of this notebook to the measured data. Here, I've used `least_squares` from `scipy.optimize`, but `curve_fit` is a more common choice. An improved curve fitting tool is in the [LMFIT](https://lmfit.github.io/lmfit-py/) package.\n",
    "\n",
    "First I define a function that takes as arguments the fitting parameters in a list (here $x$, $\\eta$ and $f_{HWHM}$), the independent variable (here $f$), and the y-variable data (here the spectrum), and outputs the residuals between data and model. Then I test the fitting using the most anti-squeezed spectrum, using initial guesses of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7880aacf-6813-4e66-b1a0-09d7fcf79ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_asq(params, freq, spec):\n",
    "    x, eta, bw = params\n",
    "    model = lin2db(Vasq(x, eta, freq/bw))\n",
    "    residuals = spec - model\n",
    "    return residuals\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff42ae05-e828-4beb-929b-1a16fe3d4ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_spec = antisqueezing_norm[2]\n",
    "least_squares(res_asq, [.5,.8,40], args=[f, test_spec])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4416342e-0e18-4ddd-8df7-c69aa15f060e",
   "metadata": {},
   "source": [
    "The error is due to the spectrum containing NaN values. These can be removed by masking them out, after which the fit works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163b3f10-5375-48af-baa4-96496c61426a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(res_asq([.5,.8,40], f, test_spec)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54385516-15fd-4efb-8279-d11a4f21bf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask1 = ~np.isnan(test_spec)\n",
    "res = least_squares(res_asq, [.5,.8,40], args=[f[mask1], test_spec[mask1]])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302cae08-57dd-4ca8-a6a8-a5547bfe0abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sq in squeezing_norm:\n",
    "    plt.plot(f, sq, c='C0')\n",
    "for asq in antisqueezing_norm:\n",
    "    plt.plot(f, asq, c='C1')\n",
    "\n",
    "x_fit, eta_fit, bw_fit = res.x\n",
    "plt.plot(f, lin2db(Vasq(x_fit, eta_fit, f/bw_fit)), 'C2')\n",
    "\n",
    "plt.ylim(-9,16)\n",
    "plt.xlim(0, 120)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d19eb2-17cc-4299-b05a-4b01a6753ed5",
   "metadata": {},
   "source": [
    "The fit is still very bad, as can be seen visually from the poor fit between data and model (green) and from the very large cost value of the fitting result. The poor fit is due to the strong signals around 40 MHz and 100 MHz and the overtone of the 40 MHz signal at 80 MHz. There's also some nasty stuff at very low frequencies. I therefore create a new mask that removes these frequencies from the data that goes into the fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c475bb-bd45-4f6e-8c4d-704040e209f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask2 = ((f>2) & (f<35)) | ((f>45) & (f<75)) | ((f>85) & (f<95))\n",
    "mask = mask1 & mask2\n",
    "len(np.nonzero(mask1 & mask2)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7abcd2-888e-4b10-ad2a-c1b332162cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = least_squares(res_asq, [.5,.8,40], args=[f[mask], test_spec[mask]])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4913c2-5ab2-40ab-9a12-bcd91deb132e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sq in squeezing_norm:\n",
    "    plt.plot(f, sq, c='C0')\n",
    "for asq in antisqueezing_norm:\n",
    "    plt.plot(f, asq, c='C1')\n",
    "\n",
    "x_fit, eta_fit, bw_fit = res.x\n",
    "plt.plot(f, lin2db(Vasq(x_fit, eta_fit, f/bw_fit)), 'C2')\n",
    "\n",
    "plt.ylim(-9,16)\n",
    "plt.xlim(0, 120)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5276013-d3e9-43f8-b6b5-9d04c4ab54d5",
   "metadata": {},
   "source": [
    "This was a really nice fit! The fitted parameters are in the `x` variable of the fitting result: `[ 7.463e-01  8.170e-01  3.622e+01]`, that is, $x=0.746$, $\\eta=0.817$, $f_{HWHM}=36.2\\text{ MHz}$.\n",
    "\n",
    "Could we perhaps make it even better by using the more complex model with phase and phase fluctuation parameters? When adding more parameters, the optimisation problem becomes more ill-defined and \"loose\", so I will add some bounds to the parameters to help it along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f58467b-b395-427b-81fa-67fd5e16c7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = least_squares(res_asq2, [.5,.8,40, 90, 0], bounds=([0,.5,10,80,0], [.99,1,50,100,10]), args=[f[mask], test_spec[mask]])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357bee26-9976-47a6-b175-ead6fafd9a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sq in squeezing_norm:\n",
    "    plt.plot(f, sq, c='C0')\n",
    "for asq in antisqueezing_norm:\n",
    "    plt.plot(f, asq, c='C1')\n",
    "\n",
    "x_fit, eta_fit, bw_fit, theta_fit, sigma_fit = res.x\n",
    "plt.plot(f, lin2db(Vrotfluct(x_fit, eta_fit, f/bw_fit, theta_fit, sigma_fit)), 'C2')\n",
    "\n",
    "plt.ylim(-9,16)\n",
    "plt.xlim(0, 120)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3159e531-098d-4ea6-8029-d2e674ecaf84",
   "metadata": {},
   "source": [
    "This doesn't work very well. The fit looks OK'ish, but two of the parameters are hitting their bounds which is a clear indication of a bad fit. Seems to be too many parameters.\n",
    "\n",
    "Next, I try to fit both the antisqueezing and squeezing for a given power at the same time. Since they're recorded with the same power and same experimental setup, the parameters $x$, $\\eta$ and $f_{HWHM}$ should be identical. I also assume $\\sigma$ to be identical (not necessarily a good assumption). To fit both curves simultaneously, one can simply concatenate values for both spectra and for both models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3675261c-66a9-42d5-bf3b-21ced120fb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "asq_spec = antisqueezing_norm[2]\n",
    "sq_spec = squeezing_norm[2]\n",
    "\n",
    "def res_asq3(params, freq, sq_spec, asq_spec):\n",
    "    x, eta, bw, theta_sq, theta_asq, sigma = params\n",
    "    sq_model = lin2db(Vrotfluct(x, eta, freq/bw, theta_sq, sigma))\n",
    "    asq_model = lin2db(Vrotfluct(x, eta, freq/bw, theta_asq, sigma))\n",
    "    model = np.concatenate([sq_model, asq_model])\n",
    "    spec = np.concatenate([sq_spec, asq_spec])\n",
    "    residuals = spec - model\n",
    "    return residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2899f2-d29b-4d99-afb8-34c2fe5ae25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = least_squares(res_asq3, [.5,.8,40, 0, 90, 0], \n",
    "                    bounds=([0,.5,10,-10,80,0], [.99,1,50,10,100,10]), \n",
    "                    args=[f[mask], sq_spec[mask], sq_spec[mask]])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9405d9-dd76-4544-a393-4cdca3345413",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sq in squeezing_norm:\n",
    "    plt.plot(f, sq, c='C0')\n",
    "for asq in antisqueezing_norm:\n",
    "    plt.plot(f, asq, c='C1')\n",
    "\n",
    "x_fit, eta_fit, bw_fit, theta_sq_fit, theta_asq_fit, sigma_fit = res.x\n",
    "plt.plot(f, lin2db(Vrotfluct(x_fit, eta_fit, f/bw_fit, theta_sq_fit, sigma_fit)), 'C3')\n",
    "plt.plot(f, lin2db(Vrotfluct(x_fit, eta_fit, f/bw_fit, theta_asq_fit, sigma_fit)), 'C2')\n",
    "\n",
    "plt.ylim(-9,16)\n",
    "plt.xlim(0, 120)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3f37bd-d157-4620-acf0-539204c200f3",
   "metadata": {},
   "source": [
    "That was a very poor fit! Let's try to remove the phase parameters and go back to basics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a58013-0b39-4f86-8966-db6bec69df3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "asq_spec = antisqueezing_norm[2]\n",
    "sq_spec = squeezing_norm[2]\n",
    "\n",
    "def res_asq3(params, freq, sq_spec, asq_spec):\n",
    "    x, eta, bw = params\n",
    "    sq_model = lin2db(Vrotfluct(x, eta, freq/bw, 0, 0))\n",
    "    asq_model = lin2db(Vrotfluct(x, eta, freq/bw, 0, 0))\n",
    "    model = np.concatenate([sq_model, asq_model])\n",
    "    spec = np.concatenate([sq_spec, asq_spec])\n",
    "    residuals = spec - model\n",
    "    return residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d6e97b-f635-4de2-8c69-20306d7a74af",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = least_squares(res_asq3, [.5,.8,40], \n",
    "                    bounds=([0,.5,10], [.99,1,50]), \n",
    "                    args=[f[mask], sq_spec[mask], sq_spec[mask]])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e511a6-40ea-46d3-b4dd-ccbd2fa347e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sq in squeezing_norm:\n",
    "    plt.plot(f, sq, c='C0')\n",
    "for asq in antisqueezing_norm:\n",
    "    plt.plot(f, asq, c='C1')\n",
    "\n",
    "x_fit, eta_fit, bw_fit = res.x\n",
    "plt.plot(f, lin2db(Vrotfluct(x_fit, eta_fit, f/bw_fit, 0, 0)), 'C3')\n",
    "plt.plot(f, lin2db(Vrotfluct(x_fit, eta_fit, f/bw_fit, 90, 0)), 'C2')\n",
    "\n",
    "plt.ylim(-9,16)\n",
    "plt.xlim(0, 120)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e4d21e-62b7-4151-b678-ac07955ce97e",
   "metadata": {},
   "source": [
    "The fit works much better now with fewer parameters. Still, for some reason, the dual fit doesn't fit well for the highest-power traces. I suspect there might be an issue with the data, or the model fails to take into account some property of the experiment. The fit for the lowest-power spectra also doesn't look amazing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13139f23-f22a-4d50-8641-496cef03ca5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "asq_spec = antisqueezing_norm[0]\n",
    "sq_spec = squeezing_norm[0]\n",
    "\n",
    "def res_asq3(params, freq, sq_spec, asq_spec):\n",
    "    x, eta, bw = params\n",
    "    sq_model = lin2db(Vrotfluct(x, eta, freq/bw, 0, 0))\n",
    "    asq_model = lin2db(Vrotfluct(x, eta, freq/bw, 0, 0))\n",
    "    model = np.concatenate([sq_model, asq_model])\n",
    "    spec = np.concatenate([sq_spec, asq_spec])\n",
    "    residuals = spec - model\n",
    "    return residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a670d9-799f-4b40-8871-b5f4103685ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = least_squares(res_asq3, [.5,.8,40], \n",
    "                    bounds=([0,.5,10], [.99,1,50]), \n",
    "                    args=[f[mask], sq_spec[mask], sq_spec[mask]])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e42ada-771b-4fd9-ab0b-b1383c7ebb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sq in squeezing_norm:\n",
    "    plt.plot(f, sq, c='C0')\n",
    "for asq in antisqueezing_norm:\n",
    "    plt.plot(f, asq, c='C1')\n",
    "\n",
    "x_fit, eta_fit, bw_fit = res.x\n",
    "plt.plot(f, lin2db(Vrotfluct(x_fit, eta_fit, f/bw_fit, 0, 0)), 'C3')\n",
    "plt.plot(f, lin2db(Vrotfluct(x_fit, eta_fit, f/bw_fit, 90, 0)), 'C2')\n",
    "\n",
    "plt.ylim(-9,16)\n",
    "plt.xlim(0, 120)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1ad1f3-08a9-43c5-83f5-2e5053aae8cd",
   "metadata": {},
   "source": [
    "However, another issue might be that measurements are a bit unreliable at higher frequencies where the detector efficiency is lowm (as seen in the first raw spectrum at the top). These have a relatively large weight in the fit and seem to skew the fit in a way that the more accurate low-frequency values don't fit well. \n",
    "\n",
    "To test this, below I made a \"manual fitting machine\" using widgets for the parameters. The default parameters inserted here work quite well for the lowest power spectra -- both squeezing and anti-squeezing -- at low frequencies, while especially the squeezing is somewhat off at higher frequencies. But this looks already much better. When increasing `x` it also somewhat matches the middle-power anti-squeezing and the highest-power squeezing - not what one would expect. It could be that the power labels in the file names are not correct, or (perhaps more likely) that the detector has some saturation at high noise powers.\n",
    "\n",
    "This manual fitting shows that there is still some massaging to do to make the optimiser give a more realistic fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d139187d-ecfd-4db7-aca7-70fc8b1ada58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "\n",
    "@interact(x=(0.,.999,.01), eta=(0.,1.,.01), bw=(10.,100.,1.), \n",
    "          theta_sq_offset=(-45.,45.,.1), theta_asq_offset=(-45.,45.,.1), sigma=(0.,10.,.01))\n",
    "def manual_fit(x=.43, eta=.9, bw=33., theta_sq_offset=0., theta_asq_offset=0., sigma=0.):\n",
    "\n",
    "    for sq in squeezing_norm:\n",
    "        plt.plot(f, sq, c='C0')\n",
    "    for asq in antisqueezing_norm:\n",
    "        plt.plot(f, asq, c='C1')\n",
    "    \n",
    "    plt.plot(f, lin2db(Vrotfluct(x, eta, f/bw, 90+theta_asq_offset, sigma)), 'C2')\n",
    "    plt.plot(f, lin2db(Vrotfluct(x, eta, f/bw, 0+theta_sq_offset, sigma)), 'C2')\n",
    "    \n",
    "    plt.ylim(-9,16)\n",
    "    plt.xlim(0, 120)\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de6b0aa-0220-4eee-9be9-c90817b6c52c",
   "metadata": {},
   "source": [
    "### Save to HDF5\n",
    "\n",
    "It is a very good idea to store data in the HDF5 format. It can contain multiple datasets, metadata, attributes on each dataset, and it is quite fast to look into even GB-large files using some of the available HDF5 viewers or e.g. the H5Web extension to VS Code. Here we format the metadata from one of the traces (using `dtype='S'` as the default `'U'` datatype for NumPy string arrays does not work) and place it in a single .hdf5 file together with all the spectra extracted from the CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3836e58e-0f45-4328-a0c5-072d31dad686",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_elec = np.genfromtxt(elec_spec_file, delimiter=',', skip_header=2, max_rows=42, dtype='S')\n",
    "meta_elec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b794bbda-f35b-4775-b5b7-4ae617854983",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_elec = {a[0]:a[1] for a in meta_elec}\n",
    "meta_elec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d942c664-ed3c-47fd-8c96-c3a6133e9104",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('squeezing_data.hdf5', 'w') as file:\n",
    "    dset_spec = file.create_group('spectrum')\n",
    "    dset_freq = dset_spec.create_dataset('frequencies', data=f)\n",
    "    dset_elec = dset_spec.create_dataset('electronic', data=electronic)\n",
    "    dset_vac = dset_spec.create_dataset('vacuum', data=vacuum)\n",
    "    dset_sqz = dset_spec.create_dataset('squeezing', data=squeezing)\n",
    "    dset_asqz = dset_spec.create_dataset('antisqueezing', data=antisqueezing)\n",
    "    for key, val in meta_elec.items():\n",
    "        dset_spec.attrs[key] = val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be9a71d-802b-4db5-966e-9fad29a41bfa",
   "metadata": {},
   "source": [
    "The data can also be compressed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c0857f-5796-4e9c-b99e-431cc002cbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('squeezing_data_zipped.hdf5', 'w') as file:\n",
    "    dset_spec = file.create_group('spectrum')\n",
    "    dset_freq = dset_spec.create_dataset('frequencies', data=f, chunks=True, compression='gzip')\n",
    "    dset_elec = dset_spec.create_dataset('electronic', data=electronic, chunks=True, compression='gzip')\n",
    "    dset_vac = dset_spec.create_dataset('vacuum', data=vacuum, chunks=True, compression='gzip')\n",
    "    dset_sqz = dset_spec.create_dataset('squeezing', data=squeezing, chunks=True, compression='gzip')\n",
    "    dset_asqz = dset_spec.create_dataset('antisqueezing', data=antisqueezing, chunks=True, compression='gzip')\n",
    "    for key, val in meta_elec.items():\n",
    "        dset_spec.attrs[key] = val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957bfa6c-af37-494d-95ce-2ef3777f26e6",
   "metadata": {},
   "source": [
    "Reading HDF5 files is also easy - just access it like a file system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a3cc53-7906-4e33-8300-a6edc6bd1fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = h5py.File('squeezing_data_zipped.hdf5', 'r')\n",
    "list(file.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f279427-0b8e-4212-9886-ffe27a1bdce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(file['spectrum'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b8ca33-b50d-4843-9fd5-f85e86d00523",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = file['spectrum']\n",
    "spec['frequencies'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fed0d6e-f233-4579-8ca3-cfbdf546b109",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = file['spectrum']\n",
    "plt.plot(spec['frequencies'][:], spec['electronic'][:])\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646c013b-45ef-4939-83d3-df4d1097b03d",
   "metadata": {},
   "source": [
    "## Xueshi data\n",
    "\n",
    "Below I give just the minimum necessary code to load Xueshi's data files. Then it's your turn to play around with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240b19d6-d006-404c-8f87-9579295c5ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['electronic', 'shotnoise', 'squeezing', 'antisqueezing']\n",
    "channels = ['C1', 'C2', 'C3', 'C4']\n",
    "\n",
    "def get_file_list(dataset, channel):\n",
    "    folder = os.path.join(xueshi_folder, dataset, channel)\n",
    "    fns = sorted(os.listdir(folder))\n",
    "    fns = [os.path.join(folder, fn) for fn in fns]\n",
    "    return fns\n",
    "\n",
    "files = {ds:\n",
    "           {ch:get_file_list(ds, ch) for ch in channels}\n",
    "           for ds in datasets}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fc6b96-45ec-45d4-8d6b-2533807e83d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta, times, data = lecroy.read(files['shotnoise']['C1'][0], scale=False)\n",
    "dt = meta['horiz_interval']\n",
    "fs = 1/dt\n",
    "N = len(data[0])\n",
    "t = np.linspace(0, fs*N, N, endpoint=False)\n",
    "f = rfftfreq(N, dt)\n",
    "# meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e7f543-15df-4e36-9eb5-9c5cac6f4303",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4c9610-e14f-4e3c-a9e0-ac6865db571b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in datasets:\n",
    "    data = np.array([[\n",
    "        lecroy.read(f, scale=False)[2] for f in files[ds][ch]]\n",
    "                     for ch in channels])\n",
    "    data = data.squeeze()\n",
    "    globals()[ds] = data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb292b5-b793-4377-8d6e-934b93b111df",
   "metadata": {},
   "outputs": [],
   "source": [
    "electronic.shape, shotnoise.shape, squeezing.shape, antisqueezing.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5404b6-f1b8-456f-83f8-1910b4131b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t, antisqueezing[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d1f20f-95e1-46fe-8683-b8c8b774fbc3",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Find the Jens data here: https://www.dropbox.com/scl/fo/3hct1gtuo9la8ysoyhuk2/AAuzFp9D7BVOpwDPJKEN_F0?rlkey=0dkbj53brwayjnj72jnzemau6&dl=0\n",
    "\n",
    "And Xueshi data here: https://www.dropbox.com/scl/fo/as7ek7z1nezg2urofje4h/AD8NKSYoHrOBiPaVM9FnJMk?rlkey=4qmt8zoeqpv1h13feffsd9ssg&dl=0\n",
    "\n",
    "1. In Jens' data, load the \"5MHz...\" files and repackage them into an HDF5 file together with metadata and the power values obtained from the file names.\n",
    "2. Reproduce the figure `power_scaling.png`: Calculate the noise variance for each of the traces and do a fit to the model for OPO squeezing variance.\n",
    "3. Load all of Xueshi's data using `lecroy.read` and repackage them into an HDF5 file.\n",
    "4. On the raw time traces of squeezing and anti-squeezing, try to create and apply a filter that can remove the strong oscillation at around 28 MHz. Look into e.g. `firwin`, `iirwin`, `lfilter` or other methods of `scipy.signal`.\n",
    "5. Calculate the averaged spectra of the data. You can consider each of the (electronic, shotnoise, squeezing, antisqueezing) to have been acquired under constant conditions, so it is okay to average all the traces for each channel. Look into `rfft` and `rfftfreq` of `scipy.fft` and/or `periodogram` and `welch` of `scipy.signal`. Look both at spectra for the individual channels and the sum of the four channels.\n",
    "6. Try to fit the model of OPO squeezing/anti-squeezing to the obtained spectra and extra fitted parameters for $x$, $\\eta$, $f_{HWHM}$ and possibly $\\theta$ or $\\sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a5a410-7b9d-4776-b583-a209d6fdae2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
